{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5fd3bbb",
   "metadata": {},
   "source": [
    "# Defining the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d204e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "# Define transition for replay buffer\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=3e-5, gamma=0.99, tau=1.5, tau_min=0.05, tau_decay=0.998):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.tau_min = tau_min\n",
    "        self.tau_decay = tau_decay\n",
    "\n",
    "        # Main network\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim)\n",
    "        )\n",
    "\n",
    "        # Target network (initially same as model)\n",
    "        self.target_model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim)\n",
    "        )\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.eval()  # Don't train target network directly\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.replay_buffer = deque(maxlen=100000)\n",
    "        self.batch_size = 128\n",
    "        self.train_step_count = 0\n",
    "        self.target_update_freq = 1000  # steps\n",
    "        self.writer = SummaryWriter(f\"runs/lunar_lander_boltzmann_{time.time()}\")\n",
    "\n",
    "    def get_action(self, state, eval_mode=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state).squeeze().numpy()\n",
    "\n",
    "        if eval_mode:\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "        exp_q = np.exp((q_values - np.max(q_values)) / self.tau)\n",
    "        probs = exp_q / np.sum(exp_q)\n",
    "        return np.random.choice(self.action_dim, p=probs)\n",
    "\n",
    "    def update_tau(self):\n",
    "        self.tau = max(self.tau_min, self.tau * self.tau_decay)\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "        self.replay_buffer.append(Transition(state, action, reward, next_state, done))\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states = torch.FloatTensor(np.array([t.state for t in batch]))\n",
    "        actions = torch.LongTensor(np.array([t.action for t in batch]))\n",
    "        rewards = torch.FloatTensor(np.array([t.reward for t in batch]))\n",
    "        next_states = torch.FloatTensor(np.array([t.next_state for t in batch]))\n",
    "        dones = torch.FloatTensor(np.array([t.done for t in batch]))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.sample_batch()\n",
    "\n",
    "        # Compute Q(s,a)\n",
    "        current_q = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "        # Compute target Q using target network\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_model(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "\n",
    "        loss = self.loss_fn(current_q.squeeze(), target_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.train_step_count += 1\n",
    "\n",
    "        # Periodically update target network\n",
    "        if self.train_step_count % self.target_update_freq == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        # Logging\n",
    "        self.writer.add_scalar('Loss/train', loss.item(), self.train_step_count)\n",
    "        self.writer.add_scalar('Temperature', self.tau, self.train_step_count)\n",
    "\n",
    "    def test_episode(self, env, render=False):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = self.get_action(state, eval_mode=True)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        return total_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7080c20b",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf6e7cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Train Reward = -345.9, Test Reward = -131.6, Tau = 0.995\n",
      "Episode 20: Train Reward = -106.7, Test Reward = -384.0, Tau = 0.900\n",
      "Episode 40: Train Reward = -258.5, Test Reward = -783.2, Tau = 0.814\n",
      "Episode 60: Train Reward = -172.7, Test Reward = -363.2, Tau = 0.737\n",
      "Episode 80: Train Reward = -262.5, Test Reward = -144.6, Tau = 0.666\n",
      "Episode 100: Train Reward = -214.9, Test Reward = -157.7, Tau = 0.603\n",
      "Episode 120: Train Reward = -58.8, Test Reward = -749.2, Tau = 0.545\n",
      "Episode 140: Train Reward = -258.4, Test Reward = -757.1, Tau = 0.493\n",
      "Episode 160: Train Reward = -276.1, Test Reward = -165.8, Tau = 0.446\n",
      "Episode 180: Train Reward = -370.7, Test Reward = -478.8, Tau = 0.404\n",
      "Episode 200: Train Reward = -57.7, Test Reward = -175.1, Tau = 0.365\n",
      "Episode 220: Train Reward = -166.4, Test Reward = -235.9, Tau = 0.330\n",
      "Episode 240: Train Reward = -140.6, Test Reward = -177.7, Tau = 0.299\n",
      "Episode 260: Train Reward = -150.0, Test Reward = -158.5, Tau = 0.270\n",
      "Episode 280: Train Reward = -2942.0, Test Reward = 23.1, Tau = 0.245\n",
      "Episode 300: Train Reward = 92.0, Test Reward = -43.5, Tau = 0.221\n",
      "Episode 320: Train Reward = 225.1, Test Reward = -173.5, Tau = 0.200\n",
      "Episode 340: Train Reward = 193.3, Test Reward = -58.1, Tau = 0.181\n",
      "Episode 360: Train Reward = 46.0, Test Reward = -424.2, Tau = 0.164\n",
      "Episode 380: Train Reward = -102.6, Test Reward = 157.1, Tau = 0.148\n",
      "Episode 400: Train Reward = 191.4, Test Reward = 19.3, Tau = 0.134\n",
      "Episode 420: Train Reward = 129.5, Test Reward = 164.6, Tau = 0.121\n",
      "Episode 440: Train Reward = 128.8, Test Reward = 184.0, Tau = 0.110\n",
      "Episode 460: Train Reward = 43.4, Test Reward = -13.3, Tau = 0.099\n",
      "Episode 480: Train Reward = 260.4, Test Reward = 220.5, Tau = 0.090\n",
      "Episode 500: Train Reward = 240.0, Test Reward = 239.7, Tau = 0.081\n",
      "Episode 520: Train Reward = 228.8, Test Reward = 210.2, Tau = 0.073\n",
      "Episode 540: Train Reward = 275.9, Test Reward = 244.5, Tau = 0.066\n",
      "Episode 560: Train Reward = 260.2, Test Reward = 219.7, Tau = 0.060\n",
      "Episode 580: Train Reward = 227.4, Test Reward = 86.7, Tau = 0.054\n",
      "Episode 600: Train Reward = 250.7, Test Reward = 274.7, Tau = 0.049\n",
      "Episode 620: Train Reward = 272.5, Test Reward = -21.0, Tau = 0.044\n",
      "Episode 640: Train Reward = 199.1, Test Reward = 271.5, Tau = 0.040\n",
      "Episode 660: Train Reward = 252.0, Test Reward = 181.4, Tau = 0.036\n",
      "Episode 680: Train Reward = -45.8, Test Reward = 263.3, Tau = 0.033\n",
      "Episode 700: Train Reward = 33.6, Test Reward = 219.8, Tau = 0.030\n",
      "Episode 720: Train Reward = 251.7, Test Reward = 265.6, Tau = 0.027\n",
      "Episode 740: Train Reward = 191.6, Test Reward = 103.6, Tau = 0.024\n",
      "Episode 760: Train Reward = 183.4, Test Reward = 292.1, Tau = 0.022\n",
      "Episode 780: Train Reward = 175.6, Test Reward = 289.6, Tau = 0.020\n",
      "Episode 800: Train Reward = 129.6, Test Reward = 185.9, Tau = 0.018\n",
      "Episode 820: Train Reward = 240.3, Test Reward = 276.5, Tau = 0.016\n",
      "Episode 840: Train Reward = 246.7, Test Reward = 224.0, Tau = 0.015\n",
      "Episode 860: Train Reward = 281.9, Test Reward = 254.1, Tau = 0.013\n",
      "Episode 880: Train Reward = 260.2, Test Reward = 262.2, Tau = 0.012\n",
      "Episode 900: Train Reward = 142.9, Test Reward = 134.5, Tau = 0.011\n",
      "Episode 920: Train Reward = 225.3, Test Reward = 138.1, Tau = 0.010\n",
      "Episode 940: Train Reward = 23.9, Test Reward = 225.5, Tau = 0.010\n",
      "Episode 960: Train Reward = 163.9, Test Reward = -407.0, Tau = 0.010\n",
      "Episode 980: Train Reward = 79.5, Test Reward = 153.3, Tau = 0.010\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v3\")\n",
    "agent = DQNAgent(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.n,\n",
    "    tau=1.0,        # Initial temperature\n",
    "    tau_min=0.01,    # Minimum temperature\n",
    "    tau_decay=0.995  # Decay rate\n",
    ")\n",
    "\n",
    "episodes = 1000\n",
    "print_interval = 20\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        \n",
    "        # Reward shaping - encourage staying centered\n",
    "        shaped_reward = reward + 0.1*(abs(state[0]) - abs(next_state[0]))\n",
    "        \n",
    "        agent.store_transition(state, action, shaped_reward, next_state, done)\n",
    "        \n",
    "        # Train every 4 steps\n",
    "        if step_count % 4 == 0:\n",
    "            agent.train_step()\n",
    "            \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "    \n",
    "    agent.update_tau()\n",
    "    agent.writer.add_scalar('Reward/train', total_reward, ep)\n",
    "    \n",
    "    if ep % print_interval == 0:\n",
    "        test_reward = agent.test_episode(env)\n",
    "        print(f\"Episode {ep}: Train Reward = {total_reward:.1f}, Test Reward = {test_reward:.1f}, Tau = {agent.tau:.3f}\")\n",
    "        agent.writer.add_scalar('Reward/test', test_reward, ep)\n",
    "\n",
    "env.close()\n",
    "agent.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5469c572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
